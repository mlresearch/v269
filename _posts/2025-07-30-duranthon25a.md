---
abstract: While graph convolutional networks show great practical promises, the theoretical
  understanding of their generalization properties as a function of the number of
  samples is still in its infancy compared to the more broadly studied case of supervised
  fully connected neural networks.  In this article, we predict the performances of
  a single-layer graph convolutional network (GCN) trained on data produced by attributed
  stochastic block models (SBMs) in the high-dimensional limit. Previously, only ridge
  regression on contextual-SBM (CSBM) has been considered in Shi et al. 2022; we generalize
  the analysis to arbitrary convex loss and regularization for the CSBM and add the
  analysis for another data model, the neural-prior SBM. We derive the optimal parameters
  of the GCN. We also study the high signal-to-noise ratio limit, detail the convergence
  rates of the GCN and show that, while consistent, it does not reach the Bayes-optimal
  rate for any of the considered cases.
openreview: ujdmmUrIra
section: Poster Presentations
title: Asymptotic Generalization Error of a Single-Layer Graph Convolutional Network
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: duranthon25a
month: 0
tex_title: Asymptotic Generalization Error of a Single-Layer Graph Convolutional Network
firstpage: '13:1'
lastpage: '13:27'
page: 13:1-13:27
order: 13
cycles: false
bibtex_author: Duranthon, O and Zdeborova, Lenka
author:
- given: O
  family: Duranthon
- given: Lenka
  family: Zdeborova
date: 2025-07-30
address:
container-title: Proceedings of the Third Learning on Graphs Conference
volume: '269'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 7
  - 30
pdf: https://raw.githubusercontent.com/mlresearch/v269/main/assets/duranthon25a/duranthon25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
