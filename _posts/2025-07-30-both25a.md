---
abstract: Many scientific problems involve energy minimization on sparse graphs, including
  such as heat diffusion in solids and synchronization. These problems often experience
  slow convergence, particularly when the graph is random, as seen in glassy systems.
  We show that graph neural networks (GNN) can be used to significantly speed up such
  optimization problems. Our idea is to represent the state of each node as the output
  of a graph neural network. We show the benefit of this GNN reparametrization in
  experiments solving heat diffusion and synchronization of nonlinear oscillators.
  When optimizing using gradient descent, we show that this GNN reparametrization
  has the effect of a quasi-Newtonâ€™s method.
openreview: 9x7XJGvfom
section: Poster Presentations
title: Faster Optimization on Sparse Graphs via Neural Reparametrization
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: both25a
month: 0
tex_title: Faster Optimization on Sparse Graphs via Neural Reparametrization
firstpage: '26:1'
lastpage: '26:21'
page: 26:1-26:21
order: 26
cycles: false
bibtex_author: Both, Csaba and Dehmamy, Nima and Long, Jianzhi and Yu, Rose
author:
- given: Csaba
  family: Both
- given: Nima
  family: Dehmamy
- given: Jianzhi
  family: Long
- given: Rose
  family: Yu
date: 2025-07-30
address:
container-title: Proceedings of the Third Learning on Graphs Conference
volume: '269'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 7
  - 30
pdf: https://raw.githubusercontent.com/mlresearch/v269/main/assets/both25a/both25a.pdf
extras:
- label: Supplementary ZIP
  link: https://raw.githubusercontent.com/mlresearch/v269/main/assets/assets/both25a/both25a-supp.zip
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
