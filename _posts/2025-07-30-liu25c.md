---
abstract: Deep graph models (e.g., graph neural networks and graph transformers) have
  become important techniques for leveraging knowledge across various types of graphs.  Yet,
  the neural scaling laws on graphs, i.e., how the performance of deep graph models
  changes with model and dataset sizes, have not been systematically investigated,
  casting doubts on the feasibility of achieving large graph models.  To fill this
  gap, we benchmark many graph datasets from different tasks and make an attempt to
  establish the neural scaling laws on graphs from both model and data perspectives.  The
  model size we investigated is up to 100 million parameters, and the dataset size
  investigated is up to 50 million samples. We first verify the validity of such laws
  on graphs, establishing proper formulations to describe the scaling behaviors.  For
  model scaling, we identify that despite the parameter numbers, the model depth also
  plays an important role in affecting the model scaling behaviors, which differs
  from observations in other domains such as CV and NLP.  For data scaling, we suggest
  that the number of graphs can not effectively measure the graph data volume in scaling
  law since the sizes of different graphs are highly irregular. Instead, we reform
  the data scaling law with the number of nodes or edges as the metric to address
  the irregular graph sizes.   We further demonstrate that the reformed law offers
  a unified view of the data scaling behaviors for various fundamental graph tasks
  including node classification, link prediction, and graph classification.  This
  work provides valuable insights into neural scaling laws on graphs, which can serve
  as an important tool for collecting new graph data and developing large graph models.
openreview: Onw7T9j4dw
section: Poster Presentations
title: Towards Neural Scaling Laws on Graphs
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: liu25c
month: 0
tex_title: Towards Neural Scaling Laws on Graphs
firstpage: '44:1'
lastpage: '44:22'
page: 44:1-44:22
order: 44
cycles: false
bibtex_author: Liu, Jingzhe and Mao, Haitao and Chen, Zhikai and Zhao, Tong and Shah,
  Neil and Tang, Jiliang
author:
- given: Jingzhe
  family: Liu
- given: Haitao
  family: Mao
- given: Zhikai
  family: Chen
- given: Tong
  family: Zhao
- given: Neil
  family: Shah
- given: Jiliang
  family: Tang
date: 2025-07-30
address:
container-title: Proceedings of the Third Learning on Graphs Conference
volume: '269'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 7
  - 30
pdf: https://raw.githubusercontent.com/mlresearch/v269/main/assets/liu25c/liu25c.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
