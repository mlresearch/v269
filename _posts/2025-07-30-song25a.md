---
abstract: Pretraining plays a pivotal role in acquiring generalized knowledge from
  large-scale data, achieving remarkable successes as evidenced by large models in
  CV and NLP. However, progress in the graph domain remains limited due to fundamental
  challenges represented by feature heterogeneity and structural heterogeneity. Recent
  efforts have been made to address feature heterogeneity via Large Language Models
  (LLMs) on text-attributed graphs (TAGs) by generating fixed-length text representations
  as node features. These high-quality features reduce the previously critical role
  of graph structure, resulting in a modest performance gap between Graph Neural Networks
  (GNNs) and structure-agnostic Multi-Layer Perceptrons (MLPs). Motivated by this,
  we introduce a feature-centric pretraining perspective by treating graph structure
  as a prior and leveraging the rich, unified feature space to learn refined interaction
  patterns that generalizes across graphs. Our framework, Graph Sequence Pretraining
  with Transformer (GSPT), samples node contexts through random walk and employs masked
  feature reconstruction to capture pairwise proximity in the LLM-unified feature
  space using a standard Transformer. By utilizing unified text representations rather
  than varying structures, GSPT alleviates structural heterogeneity and achieves significantly
  better transferability among graphs within the same domain. Our approach can be
  easily adapted to both node classification and link prediction, demonstrating promising
  empirical success on various datasets.
openreview: Uq5YzJXfrR
section: Poster Presentations
title: A Pure Transformer Pretraining Framework on Text-Attributed Graphs
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: song25a
month: 0
tex_title: A Pure Transformer Pretraining Framework on Text-Attributed Graphs
firstpage: '24:1'
lastpage: '24:18'
page: 24:1-24:18
order: 24
cycles: false
bibtex_author: Song, Yu and Mao, Haitao and Xiao, Jiachen and Liu, Jingzhe and Chen,
  Zhikai and Jin, Wei and Yang, Carl and Tang, Jiliang and Liu, Hui
author:
- given: Yu
  family: Song
- given: Haitao
  family: Mao
- given: Jiachen
  family: Xiao
- given: Jingzhe
  family: Liu
- given: Zhikai
  family: Chen
- given: Wei
  family: Jin
- given: Carl
  family: Yang
- given: Jiliang
  family: Tang
- given: Hui
  family: Liu
date: 2025-07-30
address:
container-title: Proceedings of the Third Learning on Graphs Conference
volume: '269'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 7
  - 30
pdf: https://raw.githubusercontent.com/mlresearch/v269/main/assets/song25a/song25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
