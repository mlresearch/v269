---
abstract: Self-supervised learning(SSL) is essential to obtain foundation models in
  NLP and CV domains via effectively leveraging knowledge in large-scale unlabeled
  data. The reason for its success is that a suitable SSL design can help the model
  to follow the neural scaling law, i.e., the performance consistently improves with
  increasing model and dataset sizes. However, it remains a mystery whether existing
  SSL in the graph domain can follow the scaling behavior toward building Graph Foundation
  Models~(GFMs) with large-scale pre-training. In this study, we examine whether existing
  graph SSL techniques can follow the neural scaling behavior with the potential to
  serve as the essential component for GFMs. Our benchmark includes comprehensive
  SSL technique implementations with analysis conducted on both the conventional SSL
  setting and many new settings adopted in other domains. Surprisingly, despite the
  SSL loss continuously decreasing, no existing graph SSL techniques follow the neural
  scaling behavior on the downstream performance. The model performance only merely
  fluctuates on different data scales and model scales. Instead of the scales, the
  key factors influencing the performance are the choices of model architecture and
  pretext task design. This paper examines existing SSL techniques for the feasibility
  of Graph SSL techniques in developing GFMs and opens a new direction for graph SSL
  design with the new evaluation prototype. Our code implementation is available online
  to ease reproducibility https://github.com/HaitaoMao/GraphSSLScaling.
openreview: qBTqK042Vt
section: Poster Presentations
software: https://github.com/HaitaoMao/GraphSSLScaling
title: Do Neural Scaling Laws Exist on Graph Self-Supervised Learning?
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ma25b
month: 0
tex_title: Do Neural Scaling Laws Exist on Graph Self-Supervised Learning?
firstpage: '35:1'
lastpage: '35:24'
page: 35:1-35:24
order: 35
cycles: false
bibtex_author: Ma, Qian and Mao, Haitao and Liu, Jingzhe and Zhang, Zhehua and Feng,
  Chunlin and Song, Yu and Shao, Yihan and Ma, Yao
author:
- given: Qian
  family: Ma
- given: Haitao
  family: Mao
- given: Jingzhe
  family: Liu
- given: Zhehua
  family: Zhang
- given: Chunlin
  family: Feng
- given: Yu
  family: Song
- given: Yihan
  family: Shao
- given: Yao
  family: Ma
date: 2025-07-30
address:
container-title: Proceedings of the Third Learning on Graphs Conference
volume: '269'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 7
  - 30
pdf: https://raw.githubusercontent.com/mlresearch/v269/main/assets/ma25b/ma25b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
