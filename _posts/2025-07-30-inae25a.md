---
abstract: Attribute reconstruction is used to predict node or edge features in the
  pre-training of graph neural networks. Given a large number of molecules, they learn
  to capture structural knowledge, which is transferable for various downstream property
  prediction tasks and vital in chemistry, biomedicine, and material science. Previous
  strategies that randomly select nodes to do attribute masking leverage the information
  of local neighbors. However, the over-reliance of these neighbors inhibits the modelâ€™s
  ability to learn long-range dependencies from higher-level substructures, such as
  functional groups or chemical motifs. To explicitly measure and encourage the inter-motif
  knowledge transfer in pre-trained models, we define inter-motif node influence measures
  and propose a novel motif-aware attribute masking strategy to capture long-range
  inter-motif structures by leveraging the information of atoms in neighboring motifs.
  Once each graph is decomposed into disjoint motifs, the features for every node
  within a sample motif are masked and subsequently predicted using a graph decoder.
  We evaluate our approach on eleven molecular classification and regression datasets
  and demonstrate its advantages.
openreview: orz7TcHo6e
section: Poster Presentations
title: Motif-Aware Attribute Masking for Molecular Graph Pre-Training
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: inae25a
month: 0
tex_title: Motif-Aware Attribute Masking for Molecular Graph Pre-Training
firstpage: '41:1'
lastpage: '41:15'
page: 41:1-41:15
order: 41
cycles: false
bibtex_author: Inae, Eric and Liu, Gang and Jiang, Meng
author:
- given: Eric
  family: Inae
- given: Gang
  family: Liu
- given: Meng
  family: Jiang
date: 2025-07-30
address:
container-title: Proceedings of the Third Learning on Graphs Conference
volume: '269'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 7
  - 30
pdf: https://raw.githubusercontent.com/mlresearch/v269/main/assets/inae25a/inae25a.pdf
extras:
- label: Supplementary ZIP
  link: https://raw.githubusercontent.com/mlresearch/v269/main/assets/assets/inae25a/inae25a-supp.zip
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
