---
abstract: Graph Transformers (GTs) like NAGphormer have shown impressive performance
  by encoding graph’s structural information and node features. However, their self-attention
  and complex architectures require high computation and memory, hindering their deployment.
  Thus, we propose a novel framework called Graph Transformer Distillation to Multi-Layer
  Perceptron (GraTeD-MLP). GraTeD-MLP leverages knowledge distillation (KD) and a
  novel decomposition of attentional representation to distill the learned representations
  from the teacher GT to a student MLP. During distillation, we incorporate a gated
  MLP architecture where two branches learn the decomposed attentional representation
  for a node while the third predicts node embeddings. Encoding the attentional representation
  mitigates the MLP’s over-reliance on node features, enabling robust performance
  even in inductive settings. Empirical results demonstrate that the proposed GraTeD-MLP
  has significantly faster inference time than the teacher GT model, with speed-up
  ranging from 20\texttimes -40\texttimes . With up to 25% improved performance over
  vanilla MLP. Furthermore, we empirically show that the proposed GraTeD-MLP outperforms
  other GNN distillation methods in seven datasets in both inductive and transductive
  settings
openreview: OSPc92gD7k
section: Poster Presentations
title: 'GraTeD-MLP: Efficient Node Classification via Graph Transformer Distillation
  to MLP'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: malik25a
month: 0
tex_title: 'GraTeD-MLP: Efficient Node Classification via Graph Transformer Distillation
  to MLP'
firstpage: '20:1'
lastpage: '20:15'
page: 20:1-20:15
order: 20
cycles: false
bibtex_author: Malik, Sarthak and Rai, Aditi and V, Ram Ganesh and Sehgal, Himank
  and Sethi, Akshay and Malhotra, Aakarsh
author:
- given: Sarthak
  family: Malik
- given: Aditi
  family: Rai
- given: Ram Ganesh
  family: V
- given: Himank
  family: Sehgal
- given: Akshay
  family: Sethi
- given: Aakarsh
  family: Malhotra
date: 2025-07-30
address:
container-title: Proceedings of the Third Learning on Graphs Conference
volume: '269'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 7
  - 30
pdf: https://raw.githubusercontent.com/mlresearch/v269/main/assets/malik25a/malik25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
